{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Production-Ready Preprocessing & Feature Engineering Module\n\nThis module implements a robust, scikit-learn compatible preprocessing pipeline for agricultural datasets.\nIt is designed to be modular, testable, and production-ready.\n\n**Key Components:**\n1.  **Global Configuration:** Centralized definition of feature constraints and binning logic.\n2.  **Custom Transformers:** Scikit-learn compatible classes for cleaning and feature engineering.\n3.  **Pipeline Construction:** Automated assembly of preprocessing steps.\n4.  **Verification:** Built-in unit tests to validate logic.\n\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "import logging\nimport os\nimport json\nimport numpy as np\nimport pandas as pd\nimport joblib\nfrom typing import Dict, List, Optional, Union, Tuple\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, PowerTransformer\nfrom sklearn.utils.validation import check_is_fitted\n\n# Configure Logging\n# Navigate to project root log directory\nlog_dir = \"logs\"\nif not os.path.exists(log_dir):\n    # If running from notebooks/ directory, check parent\n    if os.path.exists(\"../logs\"):\n        log_dir = \"../logs\"\n    else:\n        # Default to local logs if not found\n        os.makedirs(log_dir, exist_ok=True)\n\nlogging.basicConfig(\n    level=logging.INFO, \n    format='%(asctime)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler(os.path.join(log_dir, 'preprocessing.log')),\n        logging.StreamHandler()\n    ]\n)\nlogger = logging.getLogger(__name__)\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1. Global Configuration Block\n\nConstraints are now loaded from `config/constraints.json` to allow for flexibility across soil types.\n- **FEATURE_CONSTRAINTS**: Loaded dynamically.\n- **BIN_EDGES**: Categories for binning continuous variables.\n\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def load_constraints(config_path: str = '../config/constraints.json', profile: str = 'default') -> Dict[str, Tuple[float, float]]:\n    \"\"\"Load feature constraints from JSON config.\"\"\"\n    # Handle path relative to script location\n    if not os.path.isabs(config_path):\n        base_dir = os.path.dirname(__file__)\n        config_path = os.path.join(base_dir, config_path)\n        \n    try:\n        with open(config_path, 'r') as f:\n            config = json.load(f)\n        return config.get(profile, {})\n    except FileNotFoundError:\n        logger.warning(f\"Config file {config_path} not found. Using fallback defaults.\")\n        # Fallback defaults\n        return {\n            'N': (0, 140), 'P': (5, 145), 'K': (5, 205),\n            'temperature': (8, 45), 'humidity': (10, 100),\n            'ph': (3.5, 10.0), 'rainfall': (20, 300)\n        }\n\n# Load defaults initially\nFEATURE_CONSTRAINTS = load_constraints()\n\n# Binning logic for categorical feature generation\nBIN_EDGES = {\n    'moisture': { # Using humidity as proxy if soil_moisture missing, or for specific columns\n        'bins': [0, 30, 60, 100],\n        'labels': ['Low', 'Medium', 'High']\n    },\n    'temperature': {\n        'bins': [-np.inf, 20, 30, np.inf],\n        'labels': ['Cool', 'Moderate', 'Warm']\n    }\n}\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2. Step 1: Custom Transformers (The Logic)\n\n### DataCleaner\nHandles data integrity issues:\n- Removes duplicate rows (Critical for preventing leakage).\n- Clips values to physically possible ranges (Sensor error handling).\n\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "class DataCleaner(BaseEstimator, TransformerMixin):\n    def __init__(self, constraints: Dict[str, Tuple[float, float]] = FEATURE_CONSTRAINTS):\n        self.constraints = constraints\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        \"\"\"\n        Clean data by handling duplicates and clipping values.\n        \n        Note: Dropping duplicates in a transformer within a pipeline that requires \n        aligned X and y (like cross_validate) can be problematic. \n        This is best used in the inference pipeline or initial data cleaning.\n        \"\"\"\n        X = X.copy()\n        \n        # 1. Handle Duplicates\n        initial_shape = X.shape\n        X = X.drop_duplicates()\n        if X.shape[0] < initial_shape[0]:\n            logger.info(f\"DataCleaner: Dropped {initial_shape[0] - X.shape[0]} duplicate rows.\")\n            \n        # 2. Range Validation (Clipping)\n        for col, (min_val, max_val) in self.constraints.items():\n            if col in X.columns:\n                # Log if clipping happens\n                outliers = ((X[col] < min_val) | (X[col] > max_val)).sum()\n                if outliers > 0:\n                    logger.debug(f\"DataCleaner: Clipping {outliers} values in {col} to [{min_val}, {max_val}]\")\n                X[col] = X[col].clip(lower=min_val, upper=max_val)\n                \n        logger.info(f\"DataCleaner: Output shape {X.shape}\")\n        return X\n\n    def set_output(self, *, transform=None):\n        \"\"\"\n        Enable pandas output configuration.\n        Since this transformer always returns a DataFrame (if input is DF),\n        we just return self to allow the pipeline validation to pass.\n        \"\"\"\n        return self\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### FeatureEngineer\nImplements domain-specific feature extraction:\n- **NPK Ratios**: Critical for understanding nutrient balance.\n- **Climate Interaction**: Captures the combined effect of heat and moisture on plant stress.\n- **Nutrient Index**: A heuristic score for overall soil fertility.\n\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "class FeatureEngineer(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        X = X.copy()\n        \n        # Validate columns exist\n        required_cols = ['N', 'P', 'K', 'temperature', 'humidity']\n        if not all(col in X.columns for col in required_cols):\n            missing = [c for c in required_cols if c not in X.columns]\n            logger.warning(f\"FeatureEngineer: Missing columns {missing}. Some features won't be generated.\")\n            return X # Return as is or handle gracefully\n\n        # 1. NPK Ratios (with zero-division protection)\n        # Agricultural Logic: The ratio of nutrients defines vegetative vs reproductive growth.\n        epsilon = 1e-6\n        X['ratio_N_P'] = X['N'] / (X['P'] + epsilon)\n        X['ratio_N_K'] = X['N'] / (X['K'] + epsilon)\n        X['ratio_P_K'] = X['P'] / (X['K'] + epsilon)\n        \n        # 2. Total NPK\n        # Agricultural Logic: Total ionic concentration indicator.\n        X['total_nutrients'] = X['N'] + X['P'] + X['K']\n        \n        # 3. Climate Interaction\n        # Agricultural Logic: High Temp + High Humidity = High Disease Pressure / Heat Stress.\n        X['climate_stress_index'] = (X['temperature'] * X['humidity']) / 100.0\n        \n        # 4. Binning Logic\n        # Categorize continuous variables for decision tree stability / interpretability.\n        if 'humidity' in X.columns:\n             X['moisture_cat'] = pd.cut(\n                X['humidity'], \n                bins=BIN_EDGES['moisture']['bins'], \n                labels=BIN_EDGES['moisture']['labels']\n            ).astype(object) # Keep as object for OneHotEncoder\n            \n        if 'temperature' in X.columns:\n            X['temp_cat'] = pd.cut(\n                X['temperature'],\n                bins=BIN_EDGES['temperature']['bins'],\n                labels=BIN_EDGES['temperature']['labels']\n            ).astype(object)\n\n        # 5. Nutrient Balance Score (Liebig's Law of Minimum)\n        # Instead of a weighted sum (which implies compensation), we find the limiting factor.\n        # We normalize roughly by max expected values: N/140, P/145, K/205\n        X['nutrient_limiting_factor'] = np.minimum(\n            X['N'] / 140.0, \n            np.minimum(X['P'] / 145.0, X['K'] / 205.0)\n        )\n        \n        # Keeping 'nutrient_balance_score' for backward compatibility or different interpretation if needed, \n        # but the request specifically asked to fix the logic violation. \n        # We'll replace the old score with this new logic or rename it. \n        # Let's replace 'nutrient_balance_score' with the limiting factor value to satisfy \"fix these\".\n        X['nutrient_balance_score'] = X['nutrient_limiting_factor']\n        \n        logger.info(f\"FeatureEngineer: Generated {len(X.columns) - len(required_cols)} new features.\")\n        return X\n\n    def set_output(self, *, transform=None):\n        return self\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3. Step 2: The Scikit-Learn Pipeline (The Engine)\n\nWe coordinate the preprocessing steps:\n1. **Cleaner**: Initial sanitization.\n2. **Engineer**: Feature creation.\n3. **ColumnTransformer**: Specific handling for Numeric vs Categorical data.\n   - **Numeric**: Impute constraints -> Standard Scaler.\n   - **Categorical**: Impute missing -> One-Hot Encode.\n\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def create_training_pipeline(numeric_features: List[str], categorical_features: List[str]) -> Pipeline:\n    \"\"\"\n    Creates the full end-to-end preprocessing pipeline.\n    \"\"\"\n    \n    # Numeric Transformer Pipeline\n    # Using KNNImputer as soil properties are often correlated (e.g. pH and Rainfall).\n    # Using PowerTransformer (Yeo-Johnson) to handle skewed distributions (e.g., Rainfall, trace elements).\n    numeric_transformer = Pipeline(steps=[\n        ('imputer', KNNImputer(n_neighbors=5)),\n        ('transformer', PowerTransformer(method='yeo-johnson', standardize=True))\n    ])\n    \n    # Categorical Transformer Pipeline\n    categorical_transformer = Pipeline(steps=[\n        ('imputer', SimpleImputer(strategy='most_frequent')),\n        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n    ])\n    \n    # Column Transformer\n    # Note: FeatureEngineer adds columns, so we need to be careful with column selection.\n    # Approach: We apply FeatureEngineer FIRST, then we treat the *Resulting* columns.\n    # However, ColumnTransformer needs to know columns upfront.\n    # To make this robust: \n    # Option A: FeatureEngineer runs inside the pipeline, and we use a selector that can handle dynamic columns \n    # (available in newer sklearn via make_column_selector, but strict defining is safer for prod).\n    # Option B: We define the 'expected' output columns of FeatureEngineer.\n    \n    # For this implementation, we will assume the initial FeatureEngineer runs on the whole DataFrame,\n    # and then the ColumnTransformer selects specific columns to scale/encode.\n    \n    # Let's derive the expected columns after feature engineering\n    derived_numeric = ['ratio_N_P', 'ratio_N_K', 'ratio_P_K', 'total_nutrients', 'climate_stress_index', 'nutrient_balance_score']\n    derived_categorical = ['moisture_cat', 'temp_cat']\n    \n    full_numeric_features = numeric_features + derived_numeric\n    full_categorical_features = categorical_features + derived_categorical\n\n    preprocessor = ColumnTransformer(\n        transformers=[\n            ('num', numeric_transformer, full_numeric_features),\n            ('cat', categorical_transformer, full_categorical_features)\n        ],\n        verbose_feature_names_out=False\n    )\n    \n    # Main Pipeline\n    pipeline = Pipeline(steps=[\n        ('cleaner', DataCleaner()),\n        ('engineer', FeatureEngineer()),\n        ('preprocessor', preprocessor)\n    ])\n    \n    # Output as Pandas DataFrame for readability and easier debugging\n    pipeline.set_output(transform=\"pandas\")\n    \n    return pipeline\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4. Step 3: Logging & Persistence\n\nUtilities to save the artifacts and track execution.\n\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "class LoggingTransformer(BaseEstimator, TransformerMixin):\n    \"\"\"Wrapper to log shape and status between steps.\"\"\"\n    def __init__(self, name=\"\"):\n        self.name = name\n        \n    def fit(self, X, y=None):\n        return self\n        \n    def transform(self, X, y=None):\n        logger.info(f\"[{self.name}] Shape: {X.shape}\")\n        return X\n\ndef save_pipeline(pipeline: Pipeline, filepath: str):\n    \"\"\"Persist the pipeline to disk.\"\"\"\n    try:\n        joblib.dump(pipeline, filepath)\n        logger.info(f\"Pipeline successfully saved to {filepath}\")\n    except Exception as e:\n        logger.error(f\"Failed to save pipeline: {e}\")\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5. Step 4: Unit Tests (The Verification)\n\nThis section uses `pytest` style assertions to verify logic correctness.\nIt can be run interactively or via a test runner.\n\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def test_pipeline_logic():\n    print(\"Running Unit Tests...\")\n    \n    # Mock Data\n    data = pd.DataFrame({\n        'N': [60, 50, 400, 60],   # Note: 400 is outlier (limit 300)\n        'P': [20, 40, 40, 20],\n        'K': [55, 60, 60, 55],    # Keep K above min 50\n        'temperature': [25, 35, 10, 25],\n        'humidity': [50, 80, 45, 50],\n        'ph': [6.5, 7.0, 6.0, 6.5],\n        'rainfall': [500, 600, 450, 500] # Min 400\n    })\n    \n    # 1. Test DataCleaner (Duplicate Removal & Clipping)\n    cleaner = DataCleaner()\n    cleaned = cleaner.transform(data)\n    \n    # Check duplicate dropped (Row 0 and 3 are identical)\n    assert len(cleaned) == 3, f\"Expected 3 rows after duplicate drop, got {len(cleaned)}\"\n    \n    # Check clipping (N=200 should be clipped to 300 if > 300, or stay 200)\n    # The default config now has N max = 300. \n    # Example data has N=200, which is valid now.\n    # To test clipping, we need a value > 300.\n    # But for this existing test with 200, it should be <= 300.\n    assert cleaned['N'].max() <= 300, \"DataCleaner constraint failed\"\n    \n    # 2. Test FeatureEngineer\n    engineer = FeatureEngineer()\n    engineered = engineer.transform(cleaned)\n    \n    # Check Ratio Logic\n    # Row 0: N=60, P=20 -> N/P = 3.0\n    expected_ratio = 60 / (20 + 1e-6)\n    assert np.isclose(engineered.iloc[0]['ratio_N_P'], expected_ratio), f\"N/P Ratio calculation incorrect. Expected {expected_ratio}, got {engineered.iloc[0]['ratio_N_P']}\"\n    \n    # Check Binning\n    # Temp=25 -> 'Moderate' (20-30)\n    assert engineered.iloc[0]['temp_cat'] == 'Moderate', f\"Expected 'Moderate' temp, got {engineered.iloc[0]['temp_cat']}\"\n    \n    # 3. Test Full Pipeline\n    # Define features\n    num_vars = ['N', 'P', 'K', 'temperature', 'humidity', 'ph', 'rainfall']\n    cat_vars = [] # Start with none, relying on engineered ones\n    \n    pipeline = create_training_pipeline(num_vars, cat_vars)\n    transformed_df = pipeline.fit_transform(data)\n    \n    assert isinstance(transformed_df, pd.DataFrame), \"Pipeline output is not a DataFrame\"\n    \n    # Check Columns exist (Standard Scaled + OneHot)\n    # Scaled N, P, K... OneHot temp_cat, moisture_cat\n    expected_cols_subset = ['ratio_N_P', 'temp_cat_Moderate']\n    for col in expected_cols_subset:\n        # Note: Scaler might keep name 'ratio_N_P', OneHot makes 'temp_cat_Moderate'\n        assert any(c in transformed_df.columns for c in [col, f\"num__{col}\", f\"cat__{col}\"]), f\"Missing column {col} in output\"\n\n    print(\"[PASS] All Tests Passed!\")\n\n\ndef process_dataset(input_path: str, output_path: str, model_save_path: Optional[str] = None):\n    \"\"\"\n    Load data, process it using the pipeline, and save the result.\n    \"\"\"\n    logger.info(f\"Loading data from {input_path}...\")\n    try:\n        df = pd.read_csv(input_path)\n    except FileNotFoundError:\n        logger.error(f\"Input file not found: {input_path}\")\n        return\n\n    # Define Feature Groups based on Dataset\n    # Raw data columns: Temparature, Humidity, Moisture, Soil Type, Crop Type, Nitrogen, Potassium, Phosphorous, Fertilizer Name\n    # We need to map these to the expected N, P, K, temperature, humidity\n    \n    rename_map = {\n        'Nitrogen': 'N',\n        'Phosphorous': 'P',\n        'Potassium': 'K',\n        'Temparature': 'temperature', # Note the spelling error in source 'Temparature'\n        'Humidity': 'humidity',\n        'Moisture': 'moisture',\n        'Soil Type': 'soil_type',\n        'Crop Type': 'crop_type',\n        'Fertilizer Name': 'label' # Assuming this is the target\n    }\n    \n    df = df.rename(columns=rename_map)\n    \n    # Separate Target if present\n    target_col = 'label'\n    if target_col in df.columns:\n        y = df[target_col]\n        X = df.drop(columns=[target_col])\n    else:\n        y = None\n        X = df\n        \n    numeric_features = ['N', 'P', 'K', 'temperature', 'humidity']\n    # If we want to use moisture, we should add it.\n    if 'moisture' in X.columns:\n        numeric_features.append('moisture')\n        \n    # We have extra categorical columns now: soil_type, crop_type\n    categorical_features = []\n    if 'soil_type' in X.columns:\n        categorical_features.append('soil_type')\n    if 'crop_type' in X.columns:\n        categorical_features.append('crop_type')\n    \n    # Create Pipeline\n    pipeline = create_training_pipeline(numeric_features, categorical_features)\n    \n    # Fit & Transform\n    logger.info(\"Running preprocessing pipeline...\")\n    X_processed = pipeline.fit_transform(X)\n    \n    # If we have a target, re-attach it for the 'processed data' file, \n    # OR save X and y separately. Usually for ML pipeline readiness, \n    # saving a single dataframe with engineered features + target is convenient.\n    if y is not None:\n        # We might want to encode the target too, but usually that's done by LabelEncoder \n        # at the model training step, or we can leave it string.\n        X_processed[target_col] = y\n        \n    # Save Data\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n    X_processed.to_csv(output_path, index=False)\n    logger.info(f\"Processed data saved to {output_path}\")\n    \n    # Save Pipeline\n    if model_save_path:\n        # We refit on X to ensure the saved pipeline is fitted (redundant if using fit_transform but safe)\n        save_pipeline(pipeline, model_save_path)\n\n# Run tests if executed as script\nif __name__ == \"__main__\":\n    # 1. Run Tests\n    test_pipeline_logic()\n    \n    # 2. Process Actual Data\n    # Adjust paths relative to project root\n    raw_data_path = os.path.join(os.path.dirname(__file__), '../data/raw/crop_data.csv')\n    processed_data_path = os.path.join(os.path.dirname(__file__), '../data/processed/processed_crop_data.csv')\n    pipeline_save_path = os.path.join(os.path.dirname(__file__), '../models/preprocessing_pipeline.joblib')\n    \n    if os.path.exists(raw_data_path):\n        process_dataset(raw_data_path, processed_data_path, pipeline_save_path)\n    else:\n        logger.warning(f\"Raw data not found at {raw_data_path}. Skipping data processing.\")\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}